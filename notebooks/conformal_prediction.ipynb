{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a849ebd",
   "metadata": {},
   "source": [
    "# Conformal Prediction Minimal Implementation\n",
    "\n",
    "As a basic introduction to conformal prediction, this notebook trains a simple classifier for CIFAR10 prediction, and then adds conformal prediction on top to give prediction sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa33a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import torch\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from pprint import pprint\n",
    "from torch.utils.data import default_collate, DataLoader\n",
    "from torchvision.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc3f43",
   "metadata": {},
   "source": [
    "## Pull in the data and transform it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbc0c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_tensor(img):\n",
    "    \"\"\"Convert a PIL Image to a JAX array scaled to [0,1] or rearranged for non-MNIST images.\"\"\"\n",
    "    return torch.tensor(np.array(img), dtype=torch.float32) / 255.0\n",
    "\n",
    "def numpy_collate(batch):\n",
    "  return jax.tree.map(jnp.asarray, default_collate(batch))\n",
    "\n",
    "def one_hot(batch):\n",
    "   batch = torch.tensor(batch)\n",
    "   return torch.nn.functional.one_hot(batch, num_classes=10)\n",
    "\n",
    "dataset_gen = torch.Generator().manual_seed(42)\n",
    "\n",
    "trainvalset = CIFAR10(download=True, root='../data/', train=True, transform=pil_to_tensor, target_transform=one_hot)\n",
    "testset = CIFAR10(download=True, root='../data/', train=False, transform=pil_to_tensor, target_transform=one_hot)\n",
    "# split out a validation set\n",
    "trainset = torch.utils.data.Subset(trainvalset, indices=range(40_000))\n",
    "valset = torch.utils.data.Subset(trainvalset, indices=range(40_000, 50_000))\n",
    "# conver to dataloaders\n",
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True, generator=dataset_gen, collate_fn=numpy_collate)\n",
    "valloader = DataLoader(valset, batch_size=64, shuffle=True, generator=dataset_gen, collate_fn=numpy_collate)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True, generator=dataset_gen, collate_fn=numpy_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91fb6da",
   "metadata": {},
   "source": [
    "## Define and instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5b184cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFARClassifier(nn.Module):\n",
    "    \"\"\"Minimal CNN for CIFAR-10.\"\"\"\n",
    "    num_classes: int = 10\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray, *, train: bool = True) -> jnp.ndarray:\n",
    "        # Block 1\n",
    "        x = nn.Conv(32, (3, 3), padding='VALID')(x)      # 32×32 → 30×30, 32 ch\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Conv(64, (3, 3), padding='VALID')(x)      # 30×30 → 28×28, 64 ch\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, (2, 2), (2, 2))  # 28×28 → 14×14\n",
    "\n",
    "        # Block 2\n",
    "        x = nn.Conv(128, (3, 3), padding='VALID')(x)     # 14×14 → 12×12, 128 ch\n",
    "        x = nn.relu(x)\n",
    "        x = nn.avg_pool(x, (2, 2), (2, 2))  # 12×12 → 6×6\n",
    "\n",
    "        # Head\n",
    "        x = rearrange(x, 'b h w d -> b (h w d)')     # flatten: 6×6×128 = 4608\n",
    "        x = nn.Dense(256)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dropout(0.5, deterministic=not train)(x)\n",
    "        x = nn.Dense(self.num_classes)(x)   # logits shape: (batch, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61a8c9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:(64, 32, 32, 3), y:(64, 10)\n"
     ]
    }
   ],
   "source": [
    "for x, y in trainloader:\n",
    "    break\n",
    "\n",
    "assert type(x) is type(y) is type(jnp.ones(5))\n",
    "print(f'x:{x.shape}, y:{y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb6bf79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'params': {'Conv_0': {'bias': 32, 'kernel': 864},\n",
      "            'Conv_1': {'bias': 64, 'kernel': 18_432},\n",
      "            'Conv_2': {'bias': 128, 'kernel': 73_728},\n",
      "            'Dense_0': {'bias': 256, 'kernel': 1_179_648},\n",
      "            'Dense_1': {'bias': 10, 'kernel': 2_560}}}\n"
     ]
    }
   ],
   "source": [
    "key = jax.random.key(42)\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "model = CIFARClassifier()\n",
    "\n",
    "params = model.init(subkey, x)\n",
    "pprint(jax.tree.map(jnp.size, params), underscore_numbers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736a1bff",
   "metadata": {},
   "source": [
    "## Train the image classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a99fb866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1\n",
      "  Train loss:              1.5503\n",
      "  Validation loss:         1.2090\n",
      "  Validation accuracy:     0.5708\n",
      "\n",
      "--- Epoch 2\n",
      "  Train loss:              1.1718\n",
      "  Validation loss:         1.0383\n",
      "  Validation accuracy:     0.6416\n",
      "\n",
      "--- Epoch 3\n",
      "  Train loss:              0.9853\n",
      "  Validation loss:         0.8594\n",
      "  Validation accuracy:     0.7007\n",
      "\n",
      "--- Epoch 4\n",
      "  Train loss:              0.8761\n",
      "  Validation loss:         0.7932\n",
      "  Validation accuracy:     0.7226\n",
      "\n",
      "--- Epoch 5\n",
      "  Train loss:              0.7810\n",
      "  Validation loss:         0.7735\n",
      "  Validation accuracy:     0.7328\n",
      "\n",
      "--- Epoch 6\n",
      "  Train loss:              0.7066\n",
      "  Validation loss:         0.7603\n",
      "  Validation accuracy:     0.7357\n",
      "\n",
      "--- Epoch 7\n",
      "  Train loss:              0.6489\n",
      "  Validation loss:         0.7212\n",
      "  Validation accuracy:     0.7509\n",
      "\n",
      "--- Epoch 8\n",
      "  Train loss:              0.5827\n",
      "  Validation loss:         0.7154\n",
      "  Validation accuracy:     0.7622\n",
      "\n",
      "--- Epoch 9\n",
      "  Train loss:              0.5237\n",
      "  Validation loss:         0.7315\n",
      "  Validation accuracy:     0.7531\n",
      "\n",
      "--- Epoch 10\n",
      "  Train loss:              0.4786\n",
      "  Validation loss:         0.7772\n",
      "  Validation accuracy:     0.7485\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "from flax.training.train_state import TrainState\n",
    "\n",
    "n_epochs = 10\n",
    "lr = 1e-3\n",
    "dropout_key = jax.random.key(42)\n",
    "\n",
    "tx = optax.adamw(lr)\n",
    "state = TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "@jax.jit\n",
    "def train_step(state: TrainState, x, y, dropout_rng):\n",
    "    def loss_fn(params: jnp.ndarray):\n",
    "        logits = state.apply_fn(params, x, rngs={'dropout': dropout_rng})\n",
    "        return optax.softmax_cross_entropy(logits, y).mean()\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return loss, state\n",
    "\n",
    "@jax.jit\n",
    "def eval_step(state: TrainState, x: jnp.ndarray, y: jnp.ndarray) -> tuple[jnp.ndarray, jnp.ndarray]:\n",
    "    \"\"\"Performs a single evaluation step.\"\"\"\n",
    "    # Pass `train=False` to disable dropout\n",
    "    logits = state.apply_fn(state.params, x, train=False)\n",
    "    loss = optax.softmax_cross_entropy(logits, y).mean()\n",
    "    accuracy = jnp.mean(jnp.argmax(logits, -1) == y.argmax(-1))\n",
    "    return loss, accuracy\n",
    "\n",
    "epoch_stats = []\n",
    "for epoch in range(n_epochs):\n",
    "    print(f'--- Epoch {epoch+1}')\n",
    "    batch_losses = []\n",
    "    for x, y in trainloader:\n",
    "        dropout_key, subkey = jax.random.split(dropout_key)\n",
    "        batch_loss, state = train_step(state, x, y, subkey)\n",
    "        batch_losses.append(batch_loss)\n",
    "\n",
    "    # calculate the validation loss\n",
    "    # --- Evaluate on validation set ---\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    for x, y in valloader:\n",
    "        val_loss_batch, val_acc_batch = eval_step(state, x, y)\n",
    "        val_losses.append(val_loss_batch)\n",
    "        val_accuracies.append(val_acc_batch)\n",
    "    # --- Compute average metrics for the epoch ---\n",
    "\n",
    "    print(f'  Train loss:              {np.mean(batch_losses):.4f}')\n",
    "    print(f'  Validation loss:         {np.mean(val_losses):.4f}')\n",
    "    print(f'  Validation accuracy:     {np.mean(val_accuracies):.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "edc7f3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.plot(batch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f232b",
   "metadata": {},
   "source": [
    "### Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04d5fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import orbax.checkpoint as ocp\n",
    "from flax.training import orbax_utils\n",
    "from os.path import abspath\n",
    "# Directory to save the final model parameters\n",
    "model_dir = './checkpoints/my_trained_model'\n",
    "model_dir = abspath(model_dir)\n",
    "\n",
    "# Create a checkpointer\n",
    "param_checkpointer = ocp.PyTreeCheckpointer()\n",
    "save_args = orbax_utils.save_args_from_target(state.params)\n",
    "\n",
    "# Save only the parameters from the TrainState\n",
    "# param_checkpointer.save(model_dir, args=orbax_utils.save_args_from_target(state.params))\n",
    "param_checkpointer.save(model_dir, state.params, save_args=save_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c747d9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_params = param_checkpointer.restore(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085c4420",
   "metadata": {},
   "source": [
    "### Load a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ad38c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
